{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1478bdb",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "># **코드설명**\n",
    "\n",
    "---\n",
    "\n",
    "- 파 일 명 :  <br>\n",
    "- 시작날짜 :  <br>\n",
    "- 수정날짜 :  <br>\n",
    "- 작 성 자 : 김혁진 <br>\n",
    "- 작성주제 :  <br>\n",
    "\n",
    "--- \n",
    "\n",
    "- **참조**\n",
    "\n",
    "  (1) 대회 홈페이지 : [Dacon](https://dacon.io/competitions/official/235848/overview/description) <br>\n",
    "  (2) 하이퍼 파리미터 설명 : [Naver Blog](https://blog.naver.com/wideeyed/221333529176) <br>\n",
    "  (3) Class문 설명 : [Github](https://zzsza.github.io/development/2020/07/05/python-class/) <br>\n",
    "  (4) GPU 설정 : [Medium](https://medium.com/@am.sharma/lgbm-on-colab-with-gpu-c1c09e83f2af) <br>\n",
    "  (5) RAM 모두사용으로 세션다운 : [Tistory](https://somjang.tistory.com/entry/Google-Colab-%EC%9E%90%EC%A3%BC%EB%81%8A%EA%B8%B0%EB%8A%94-%EB%9F%B0%ED%83%80%EC%9E%84-%EB%B0%A9%EC%A7%80%ED%95%98%EA%B8%B0)\n",
    "\n",
    "---\n",
    "\n",
    "- **고려사항** <br>\n",
    "  (1) AutoEncoder로 파생변수 생성해보기 <br>\n",
    "  (2) 하이퍼파라미터 탐색 : grid-search, bayesian-optimization, [optuna](https://dacon.io/competitions/official/235713/codeshare/2704?page=1&dtype=recent)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8eb9c1",
   "metadata": {},
   "source": [
    "># **기본설정**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c2214",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Markdown : Tabular Left Align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a624da7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f52129",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Jupyter Notebook Style : Theme, Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845b639a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # theme 설치\n",
    "# !pip install jupyterthemes\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade notebook\n",
    "\n",
    "# # jupyter notebook 최신버전\n",
    "# !pip install --upgrade jupyterthemes\n",
    "\n",
    "# 2.2.1. 테마바꾸기(customizing)\n",
    "# !jt -t onedork -fs 115 -nfs 125 -tfs 115 -dfs 115 -ofs 115 -cursc r -cellw 80% -lineh 115 -altmd  -kl -T -N\n",
    "\n",
    "# 2.2.2. 쥬피터 노트북 화면 넓게 사용\n",
    "# 출처: https://taehooh.tistory.com/entry/Jupyter-Notebook-주피터노트북-화면-넓게-쓰는방법\n",
    "# from IPython.core.display import display, HTML \n",
    "# display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "# # 2.2.3. 좌측 TOC 만들기\n",
    "# # 출처 : https://gmnam.tistory.com/246\n",
    "# pip install jupyter_nbextensions_configurator\n",
    "# pip install jupyter_contrib_nbextensions\n",
    "\n",
    "# jupyter nbextensions_configurator enable --user\n",
    "# jupyter contrib nbextension install --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cecfd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2.3.1 Google Drive Mount\n",
    "# # (Google Drive 사용 시 설정)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True) # 새로운 창에서 key 를 받아서 입력해야합니다. \n",
    "\n",
    "# # 2.3.2. 메모리 에러\n",
    "# https://growingsaja.tistory.com/477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3b8cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2.3.3. GPU 사용 (6분)\n",
    "# !git clone --recursive https://github.com/Microsoft/LightGBM\n",
    "# !mkdir build\n",
    "# %cd /content/LightGBM\n",
    "# !cmake -DUSE_GPU=1 #avoid ..\n",
    "# !make -j$(nproc)\n",
    "# !sudo apt-get -y install python-pip\n",
    "# !sudo -H pip install setuptools pandas numpy scipy scikit-learn -U\n",
    "# %cd /content/LightGBM/python-package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196e8d53",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Install Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b98c6112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall pandas -y\n",
    "# !pip uninstall numpy  -y\n",
    "# !pip uninstall lightgbm -y\n",
    "\n",
    "# !pip install pandas==1.1.0\n",
    "# !pip install numpy==1.21.2\n",
    "# !pip install -U scikit-learn\n",
    "# !pip install lightgbm --install-option=--gpu\n",
    "\n",
    "# !pip install pandasql\n",
    "# !pip install seaborn\n",
    "# !pip install plotnine\n",
    "# !pip install pandasql\n",
    "\n",
    "# lightgbm 에러떴는데, 콘다에서 실행하면 해결됨\n",
    "# conda install -c conda-forge lightgbm \n",
    "\n",
    "# bayesian optimization 설치\n",
    "# !pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a02e69",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c6ebbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jupyter notebook 전용\n",
    "from tqdm.notebook import tqdm\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# basic modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "\n",
    "# value_counts() 범용적인 버전\n",
    "from collections import Counter as cnt\n",
    "\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(11.7, 8.27)})\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [11.7, 8.27] # [15, 10] # [11.7,8.27] - A4 size\n",
    "\n",
    "from plotnine import *\n",
    "\n",
    "\n",
    "# sqldf\n",
    "from pandasql import sqldf\n",
    "sql = lambda q: sqldf(q, globals())\n",
    "\n",
    "\n",
    "# modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# import lightgbm\n",
    "# !pip install lightgbm --install-option=--gpu --install-option=\"--opencl-include-dir=/usr/local/cuda/include/\" --install-option=\"--opencl-library=/usr/local/cuda/lib64/libOpenCL.so\"\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "# Hyperparameter Optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099b6ae",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Initial Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad47db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.4.1. Data Path\n",
    "# jupyter.notebook : 'os.getcwd() + '/DAT/블랙 프라이데이 판매 예측/''\n",
    "# google.colab     : '/content/drive/MyDrive/Python/4. 블랙프라이데이 판매예측/DAT/'\n",
    "DATA_PATH = os.getcwd() + '/DAT/2. 심장 질환 예측 경진대회 (Dacon)/'\n",
    "OUT_PATH  = os.getcwd() + '/OUT/2. 심장 질환 예측 경진대회 (Dacon)/'\n",
    "\n",
    "# 2.4.2. set seed\n",
    "SEED = 777\n",
    "\n",
    "# 2.4.3. plot\n",
    "PLOT = True\n",
    "\n",
    "# 2.4.4. scaling\n",
    "SCALE = True\n",
    "\n",
    "# 2.4.5. interaction term\n",
    "INTERACTION = True # True\n",
    "\n",
    "# initial value save\n",
    "ini_var = ['SEED','PLOT','SCALE','INTERACTION']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd87120",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Set Off the Warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a3fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb769937",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## User Defined Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0a3e77b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.1. Seed Fix\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def seed_everything(seed: int = 1):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    # torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    # torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "    \n",
    "seed_everything(SEED)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.2. View all columns\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def View(data):\n",
    "\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 500)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    \n",
    "    print(data)\n",
    "\n",
    "    pd.set_option('display.max_rows', 0)\n",
    "    pd.set_option('display.max_columns', 0)\n",
    "    pd.set_option('display.width', 0)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.3. minmax function\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def minmax(x):\n",
    "    return min(x),max(x)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.4. 컬럼dict에서 target 제거\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - dict : 기준 dict\n",
    "# - key  : 삭제할 key\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def rmkey(dict, key):\n",
    "    tmp = dict.copy()\n",
    "    del tmp[key]\n",
    "    return tmp\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.5. 각 컬럼의 missing 개수를 파악하는 함수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - col_type : {column명 : type}로 이루어진 dictionary\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def missing_column_check(data, col_type):\n",
    "    n_na = []\n",
    "    n_na_type = []\n",
    "    for col_nm in data.columns:\n",
    "        data[col_nm] = data[col_nm].astype(col_type[col_nm])\n",
    "\n",
    "        # str인 경우에는 blank(공백)도 있는지 확인\n",
    "        if col_type[col_nm]==str:\n",
    "\n",
    "            isnull_cnt = data[col_nm].str.strip().isnull().sum()\n",
    "            blank_cnt  = sum(data[col_nm].str.strip()=='')\n",
    "            nan_cnt    = sum(data[col_nm].str.strip()=='nan')\n",
    "            null_cnt   = sum(data[col_nm].str.strip()=='null')\n",
    "\n",
    "            n_na_x = isnull_cnt+blank_cnt+nan_cnt+null_cnt\n",
    "            n_na.append(n_na_x)\n",
    "            \n",
    "            if n_na_x>0:\n",
    "                n_na_type_x=[]\n",
    "                if isnull_cnt>0: n_na_type_x.append('isnull')\n",
    "                if blank_cnt >0: n_na_type_x.append('blank')\n",
    "                if nan_cnt   >0: n_na_type_x.append('nan')\n",
    "                if null_cnt  >0: n_na_type_x.append('null')\n",
    "                n_na_type_x = '+'.join(n_na_type_x)\n",
    "            else:\n",
    "                n_na_type_x = ''\n",
    "            n_na_type.append(n_na_type_x)\n",
    "            \n",
    "\n",
    "        # numeric인 경우에는 null의 개수만 확인\n",
    "        else:\n",
    "            n_na_x = data[col_nm].isnull().sum()\n",
    "            n_na.append(n_na_x)\n",
    "            \n",
    "            if n_na_x>0:\n",
    "                n_na_type.append('isnull')\n",
    "            else:\n",
    "                n_na_type.append('')\n",
    "            \n",
    "    res_df = pd.DataFrame({\n",
    "        'col'  : data.columns,\n",
    "        'n_na' : n_na,\n",
    "        'n_n_ratio' : [str(round(n/len(data)*100,1))+'%' for n in n_na],\n",
    "        'na_type' : n_na_type,\n",
    "        'col_type' : [COL_TYPE[col].__name__ for col in data.columns]\n",
    "        })\n",
    "\n",
    "    res_df = res_df[res_df['n_na']>0]\n",
    "    if len(res_df)==0:\n",
    "        return('Dataset does not have a null value')\n",
    "    else:\n",
    "        return(res_df)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.6. 교호작용항 추가\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data     : 기준 data\n",
    "# - num_vari : 숫자형 변수 list\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def interaction_term(data,num_vari):\n",
    "\n",
    "    num_var = list(set(num_vari) - set(['id']))\n",
    "\n",
    "    for i in range(0,len(num_var)):\n",
    "        for j in range(i,len(num_var)):\n",
    "            data[f'{num_var[i]}*{num_var[j]}'] = data[f'{num_var[i]}']*data[f'{num_var[j]}']\n",
    "\n",
    "    return(data)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.7. color when print\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "class color:\n",
    "    PURPLE    = '\\033[95m'\n",
    "    CYAN      = '\\033[96m'\n",
    "    DARKCYAN  = '\\033[36m'\n",
    "    BLUE      = '\\033[94m'\n",
    "    GREEN     = '\\033[92m'\n",
    "    YELLOW    = '\\033[93m'\n",
    "    RED       = '\\033[91m'\n",
    "    BOLD      = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END       = '\\033[0m'\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.8. density plot : histogram + density plot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# - data : 기준 data\n",
    "# - vars : hist + kde를 그릴 숫자형 변수\n",
    "# - hue  : group화 변수\n",
    "# - binwidth_adj_ratio : binwidth 조정 비율\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def density_plot(data, vars, \n",
    "                 binwidths = None, hue = None,\n",
    "                 binwidth_adj_ratio = None):\n",
    "\n",
    "    from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "    # 1) vars가 1개뿐일 때 에러발생\n",
    "    #    -> 1개     : type = str\n",
    "    #    -> 2개이상 : type = ndarray, ...\n",
    "    if type(vars)==str:\n",
    "        vars = [vars]\n",
    "    \n",
    "    # 2) plotting (nrow,ncol) 설정\n",
    "    nrow = math.ceil(len(vars)**(1/2))\n",
    "    ncol = nrow\n",
    "\n",
    "    # 3) binwidths가 없을 때, binwidth 설정\n",
    "    # 출처 : http://www.aistudy.co.kr/paper/pdf/histogram_jeon.pdf\n",
    "    if binwidths is None:\n",
    "        binwidths = []\n",
    "        for col in data[vars].columns:\n",
    "            n_bin = math.ceil(1 + 3.32*math.log10(len(data)))\n",
    "            binwidth = ( data[col].max() - train[col].min() ) / n_bin\n",
    "            binwidths.append(binwidth)\n",
    "            del binwidth\n",
    "    \n",
    "    # 4) 설정한 binwidth를 조정하는 비율\n",
    "    if binwidth_adj_ratio is not None:\n",
    "        binwidths = [binwidth * binwidth_adj_ratio for binwidth in binwidths]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    # 5) vars 별로 plot 생성\n",
    "    for iter,var in enumerate(vars):\n",
    "        \n",
    "        binwidth = binwidths[iter]\n",
    "        \n",
    "        # (1) histogram\n",
    "        ax1 = fig.add_subplot(nrow, ncol, iter+1)\n",
    "        g1 = sns.histplot(data = data, x = var, hue = hue,\n",
    "                          kde = True, stat = 'probability', \n",
    "                          color = 'lightskyblue',\n",
    "                          binwidth = binwidth, ax = ax1)\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        # (2) density plot\n",
    "        g2 = sns.kdeplot(data = data, x = var, hue = hue,\n",
    "                         color = 'red', lw = 2, ax = ax2)\n",
    "        ax2.set_ylim(0, ax1.get_ylim()[1] / binwidth)                  # similir limits on the y-axis to align the plots\n",
    "        #ax2.yaxis.set_major_formatter(PercentFormatter(1 / binwidth))  # show axis such that 1/binwidth corresponds to 100%\n",
    "        ax2.grid(False)\n",
    "        \n",
    "        # (3) density plot y축 없애기\n",
    "        g2.set(yticklabels=[]) \n",
    "        g2.set(ylabel=None)\n",
    "        g2.tick_params(right=False)\n",
    "        \n",
    "        a,b = divmod(iter,ncol)\n",
    "        if b!=0:\n",
    "            g1.set(ylabel=None)\n",
    "        \n",
    "    # 안겹치도록 설정\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# example : density_plot(train, vars=num_vari)\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "# 2.6.9. density plot : histogram + density plot\n",
    "#\n",
    "# (1) grp_var vs hue_var 막대그래프\n",
    "# (2) grp_var(x축), hue_var에 따른 각 num_var들의 barplot, violineplot, box+swarmplot + kdeplot\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "#- grp_var : x축 구분할 그룹변수 (text)\n",
    "#- num_vari : 숫자형 변수 (list)\n",
    "#- data : 기준 data\n",
    "#- title_text : plot title (text)\n",
    "#- hue_var : hue 그루핑변수\n",
    "#-------------------------------------------------------------------------------------------------------#\n",
    "def plot_num(grp_var, num_vari, data, title_text=None, hue_var=None):\n",
    "    \n",
    "    # (1)번 그래프 setting\n",
    "    fig0 = plt.figure(figsize=(3,3))\n",
    "    ax0  = fig0.add_subplot(1,1,1)\n",
    "    \n",
    "    if title_text is None:\n",
    "        title_text = grp_var\n",
    "        \n",
    "    plt.title(title_text, loc='left', pad=20, fontdict={'fontsize' : 30,\n",
    "                                                        'fontweight' : 'bold',\n",
    "                                                        'color' : 'c'})\n",
    "    \n",
    "    # grp_var와 hue_var가 겹치는 경우, hue를 나누지 않음\n",
    "    if (grp_var!=hue_var) and (hue_var is not None):\n",
    "\n",
    "        ct = pd.crosstab(data[grp_var],data[hue_var])\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "        ax.legend(title=hue_var, bbox_to_anchor=(1, 1.02), loc='upper left')\n",
    "        \n",
    "    else:\n",
    "        ct = data[grp_var].value_counts()\n",
    "        ax = ct.plot(kind='bar', stacked=False, rot=0, ax=ax0)\n",
    "        \n",
    "    # show\n",
    "    plt.xlabel('')\n",
    "    plt.show()\n",
    "    \n",
    "    # 숫자변수중에 [grp,id]변수가 있으면 제외\n",
    "    num_vari_x = list(set(num_vari) - set([grp_var,'id']))\n",
    "    \n",
    "    # plt 생성\n",
    "    fig = plt.figure(figsize=(15,15)) # figsize=(15,7)\n",
    "    plt.axis('off') # 안끄면 x축에 0~1까지 축생김\n",
    "    \n",
    "    for iter,var in enumerate(num_vari_x):\n",
    "\n",
    "        # hue랑 grp_var랑 같으면 hue를 넣지않음\n",
    "        hue_x = [None if grp_var==hue_var else hue_var][0]\n",
    "\n",
    "        # (n,4) plot\n",
    "        ax1 = fig.add_subplot(len(num_vari_x),4,4*iter+1)\n",
    "        ax2 = fig.add_subplot(len(num_vari_x),4,4*iter+2)\n",
    "        ax3 = fig.add_subplot(len(num_vari_x),4,4*iter+3)\n",
    "        ax4 = fig.add_subplot(len(num_vari_x),4,4*iter+4)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-1) 3번째 : box + swarm plot (ylim가져오기위해서 제일 먼저 실행)\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        g11=sns.swarmplot(x=grp_var, y=var, data=data, ax = ax3, color='crimson', marker='*', s = 7)\n",
    "        g12=sns.boxplot  (x=grp_var, y=var, data=data, ax = ax3)\n",
    "        g12.set(ylabel=None)\n",
    "        g12.set(yticklabels=[])\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-2) 1번째 : barplot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax1.set_ylim(ax3.get_ylim())\n",
    "        g21=sns.barplot(x=grp_var, y=var, data=data, ax=ax1, hue=hue_x)\n",
    "        # g21.set(ylabel=None)\n",
    "        # g21.set(yticklabels=[])\n",
    "        # g21.axes.set_title(str(iter+1) + ':' + var, fontsize=20, weight='bold', ha='left', x=-.05)\n",
    "        g21.set_ylabel(var,fontsize=20)\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-3) 2번째 : violinplot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax2.set_ylim(ax3.get_ylim())\n",
    "        g31=sns.violinplot(x=grp_var, y=var, data=data, ax=ax2, legend=False, hue=hue_x)\n",
    "        g31.set(ylabel=None)\n",
    "        g31.set(yticklabels=[])\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        # (2-4) 4번째 : density plot\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        ax4.set_ylim(ax3.get_ylim())\n",
    "        g41=sns.kdeplot(y=var, hue=grp_var, data=data, ax=ax4)\n",
    "        g41.set(ylabel=None)\n",
    "        g41.set(yticklabels=[])\n",
    "        g41.tick_params(right=False)\n",
    "        g41.set(xlabel=None)\n",
    "        g41.set(xticklabels=[])\n",
    "        \n",
    "        # 맨 아래에만 x축이 생성되도록 setting\n",
    "        if (iter+1) != len(num_vari_x):\n",
    "            \n",
    "            g12.set(xlabel=None)\n",
    "            g12.set(xticklabels=[])\n",
    "\n",
    "            g21.set(xlabel=None)\n",
    "            g21.set(xticklabels=[])\n",
    "\n",
    "            g31.set(xlabel=None)\n",
    "            g31.set(xticklabels=[])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# # example\n",
    "# plot_num(grp_var = 'sex', num_vari = num_vari, hue_var = 'target',\n",
    "#          data = train, title_text = 'sex')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5f20af",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32e1f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053f33a6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Data**\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## 변수정보 (변수명 참조 : [Dacon](https://dacon.io/competitions/official/235848/data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28384f",
   "metadata": {},
   "source": [
    "|변수명 | 변수정보 | 기준 | 변수상세 |\n",
    "|:---:|:---|:---|:---|\n",
    "| id | 데이터 고유 id | | |\n",
    "| age | 나이 | | |\n",
    "| sex | 성별  | 여자 = 0, 남자 = 1 | | |\n",
    "| cp | 가슴 통증 종류 | 무증상 = 0, 일반적이지 않은 협심증 = 1, 협심증이 아닌 통증 = 2, 일반적인 협심증 = 3 | | |\n",
    "| trestbps | 휴식 중 혈압(mmHg) | | | resting blood pressure |\n",
    "| chol | 혈중 콜레스테롤(mg/dl) | | serum cholestoral |\n",
    "| fbs | 공복 중 혈당 | 120 mg/dl 이하일 시 = 0, 초과일 시 = 1 | | fasting blood sugar |\n",
    "| restecg | 휴식 중 심전도 결과 | 좌심실비대증이 의심되거나 확실한 경우 = 0, 정상 = 1, having ST-T wave abnormality = 2 | resting electrocardiographic |\n",
    "| thalach | 최대 심박수 | | maximum heart rate achieved |\n",
    "| exang | 활동으로 인한  협심증 여부 | 없음 = 0, 있음 = 1 | exercise induced angina |\n",
    "| oldpeak | 휴식 대비 운동으로 인한 ST 하강 | | ST depression induced by exercise relative to rest |\n",
    "| slope | 활동 ST 분절 피크의 기울기 | 하강 = 0, 평탄 = 1, 상승 = 2 | the slope of the peak exercise ST segment |\n",
    "| ca | 형광 투시로 확인된 주요 혈관 수 | 0~3 개, <strong style=\"color:red\">Null값은 4로 인코딩됨</strong> | number of major vessels colored by flouroscopy |\n",
    "| thal | 지중해빈혈 여부 | 정상 = 1, 고정 결함 = 2, 가역 결함 = 3, <strong style=\"color:red\">Null값은 0으로 인코딩됨</strong> | thalassemia |\n",
    "| target | 심장 질환 진단 여부 | 혈관 지름 축소 50% 미만 = 0, 혈관 지름 축소 50% 이상 = 1 | |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda586bc",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d4c0dc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>197</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>152</td>\n",
       "      <td>298</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>283</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>140</td>\n",
       "      <td>233</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>148</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>227</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>160</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>150</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>151</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  age sex cp  trestbps  chol fbs restecg  thalach exang  oldpeak  \\\n",
       "0      1   53   1  2       130   197   1       0      152     0      1.2   \n",
       "1      2   52   1  3       152   298   1       1      178     0      1.2   \n",
       "2      3   54   1  1       192   283   0       0      195     0      0.0   \n",
       "3      4   45   0  0       138   236   0       0      152     1      0.2   \n",
       "4      5   35   1  1       122   192   0       1      174     0      0.0   \n",
       "..   ...  ...  .. ..       ...   ...  ..     ...      ...   ...      ...   \n",
       "146  147   50   1  2       140   233   0       1      163     0      0.6   \n",
       "147  148   51   1  2        94   227   0       1      154     1      0.0   \n",
       "148  149   69   1  3       160   234   1       0      131     0      0.1   \n",
       "149  150   46   1  0       120   249   0       0      144     0      0.8   \n",
       "150  151   63   0  1       140   195   0       1      179     0      0.0   \n",
       "\n",
       "    slope ca thal target  \n",
       "0       0  0    2      1  \n",
       "1       1  0    3      1  \n",
       "2       2  1    3      0  \n",
       "3       1  0    2      1  \n",
       "4       2  0    2      1  \n",
       "..    ... ..  ...    ...  \n",
       "146     1  1    3      0  \n",
       "147     2  1    3      1  \n",
       "148     1  1    2      1  \n",
       "149     2  0    3      0  \n",
       "150     2  2    2      1  \n",
       "\n",
       "[151 rows x 15 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COL_TYPE = {\n",
    "    'id'       : int,\n",
    "    'age'      : int,\n",
    "    'sex'      : str,    # 0,1\n",
    "    'cp'       : str,    # 0,1,2,3\n",
    "    'trestbps' : int,\n",
    "    'chol'     : int,\n",
    "    'fbs'      : str,    # 0,1\n",
    "    'restecg'  : str,    # 0,1,2\n",
    "    'thalach'  : int,\n",
    "    'exang'    : str,    # 0,1\n",
    "    'oldpeak'  : float,\n",
    "    'slope'    : str,    # 0,1,2\n",
    "    'ca'       : str,    # 0~3이고, Null=4\n",
    "    'thal'     : str,    # 1,2,3이고, Null=0\n",
    "    'target'   : str,    # 0,1\n",
    "}\n",
    "\n",
    "# Train Data Load (550,068 rows, 12 columns)\n",
    "train = pd.read_csv(DATA_PATH + 'train.csv', dtype = COL_TYPE)\n",
    "test  = pd.read_csv(DATA_PATH + 'test.csv', dtype = COL_TYPE)\n",
    "sub   = pd.read_csv(DATA_PATH + 'sample_submission.csv', dtype = COL_TYPE)\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4ebf0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Missing Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6e0e065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> # of train Missing : 0\n",
      "> # of test  Missing : 0\n"
     ]
    }
   ],
   "source": [
    "print(color.BOLD + color.BLUE + '> # of train Missing : \\n' + color.END, missing_column_check(train, COL_TYPE), '\\n')\n",
    "print(color.BOLD + color.BLUE + '> # of test  Missing : \\n' + color.END, missing_column_check(test , COL_TYPE), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ef1c0",
   "metadata": {},
   "source": [
    "##### Missing 값은 없음.\n",
    "##### 이외에 Null을 인위적으로 다른 값으로 인코딩한 값들 확인\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ccd1b8",
   "metadata": {},
   "source": [
    "##### 정확한 Missing value 분석은 EDA에서 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995d1761",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f5e31a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 변수 : 15\n",
      "숫자 변수 : 6\n",
      "문자 변수 : 9\n"
     ]
    }
   ],
   "source": [
    "# 숫자형변수, 문자형변수\n",
    "num_vari  = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [int,float]]\n",
    "char_vari = [key for key in COL_TYPE.keys() if COL_TYPE[key] in [str      ]]\n",
    "\n",
    "num_df  = train[num_vari]\n",
    "char_df = train[char_vari]\n",
    "\n",
    "print('전체 변수 :', len(train.columns))\n",
    "print('숫자 변수 :', len(num_vari))\n",
    "print('문자 변수 :', len(char_vari))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc21bfe7",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3d449a",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764bd50",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### hist + kde plot (hue=target) : 각 숫자형변수별 분포 확인 & target에 따른 분포확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d559f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    # No hue\n",
    "    print(color.BOLD + color.BLUE + '> No Group' + color.END)\n",
    "    density_plot(train,\n",
    "                 vars = set(num_vari) - set(['id']),\n",
    "                 binwidth_adj_ratio = 0.8)\n",
    "    plt.show()\n",
    "\n",
    "    # hue : target\n",
    "    print(color.BOLD + color.BLUE + '> Group by Target' + color.END)\n",
    "    density_plot(train,\n",
    "                 vars = set(num_vari) - set(['id']),\n",
    "                 hue = 'target',\n",
    "                 binwidth_adj_ratio = 0.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897d4424",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### Pairplot : 숫자형변수간의 관계파악"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51584c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    pairplot_df = num_df.copy().drop(['id'],axis=1)\n",
    "\n",
    "    sns.pairplot(pd.concat([pairplot_df, train.target], axis = 1),\n",
    "                 corner=True, hue = 'target')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed0b992",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## Numeric Variable * Characteristic Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a0edb",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea829f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    for _iter,_col in enumerate(sorted(char_vari)):\n",
    "        plot_num(grp_var = _col, num_vari = num_vari, hue_var='target',\n",
    "                 data = train,\n",
    "                 title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef1884",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027b8edc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if PLOT:\n",
    "\n",
    "    tmp_vari = list(set(char_vari) - set(['target']))\n",
    "\n",
    "    for _iter,_col in enumerate(sorted(tmp_vari)):\n",
    "        plot_num(grp_var = _col, num_vari = num_vari,\n",
    "                 data = test, hue_var=None,\n",
    "                 title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1440262e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Segment** : segment를 구분하여 따로 모델 적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4df7c88",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 각 조합별 건수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2c5aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segment0 = pd.Series(['1' if ca=='0' else '0' for ca in train.ca])\n",
    "segment1 = pd.Series(['1' if cp=='0' else '0' for cp in train.cp])\n",
    "segment2 = train.sex\n",
    "segment3 = train.exang\n",
    "segment4 = pd.Series(['1' if slope=='2' else '0' for slope in train.slope])\n",
    "\n",
    "def comb_seg(n_comb, head):\n",
    "\n",
    "    import itertools\n",
    "\n",
    "    # 각 조합에 대해 건수를 추출\n",
    "    res = []\n",
    "    combination = list(itertools.product([1, 0], repeat=5))\n",
    "    for comb in combination:\n",
    "        \n",
    "        # n_comb와 맞는 것들만 추출\n",
    "        if sum(comb)==n_comb:\n",
    "            comb_number = np.where(np.array(comb)==1)[0].tolist()\n",
    "            comb_seg_ele = ['segment' + str(c) for c in comb_number]\n",
    "            comb_seg = eval('+'.join(comb_seg_ele))\n",
    "\n",
    "            ct = np.array(list(cnt(comb_seg).values()))\n",
    "            ct = ct.flatten()\n",
    "            ct = np.unique(ct)\n",
    "            \n",
    "            res.append(ct)\n",
    "\n",
    "    res = np.array(res)\n",
    "    res_shape = res.shape\n",
    "    \n",
    "    # level이 달라서, flatten이 안되는 경우\n",
    "    # -> 1행의 list로 변환\n",
    "    if len(res_shape)==1:\n",
    "        ret = []\n",
    "        for r in res:\n",
    "            for ele in r:\n",
    "                ret.append(ele)\n",
    "        \n",
    "        res = ret\n",
    "        \n",
    "    else:\n",
    "        res = res.flatten()\n",
    "        \n",
    "    # unique, sort, head\n",
    "    res = np.unique(sorted(res))[:head]\n",
    "\n",
    "    return(res)\n",
    "\n",
    "print('  iter : min(1st, 2nd, 3rd, ...)')\n",
    "print('-'*40)\n",
    "for iter in range(1,6):\n",
    "    print(f'     {iter} : {comb_seg(iter,head=7)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95081ea5",
   "metadata": {},
   "source": [
    "##### 2개 조합부터는 건수가 20개 이하인 Segment가 있는데, 건수가 적절하지 않아보임.\n",
    "##### 1개씩 Segment로 사용하는 것이 좋아보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df41380",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3a24cf",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 건수 적은 변수들 합치기 & 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a655351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(_df):\n",
    "    df = _df.copy()\n",
    "    \n",
    "    #------------------------------------------------------------#\n",
    "    # 1. \n",
    "    #------------------------------------------------------------#\n",
    "    \n",
    "    return df\n",
    "\n",
    "train2 = preprocessing(train.copy())\n",
    "test2  = preprocessing(test .copy())\n",
    "\n",
    "# col type에 추가\n",
    "for str_var in []:\n",
    "    COL_TYPE[str_var] = str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a56cf2",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 추가한 변수에 대해서 EDA 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26205f6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _iter,_col in enumerate(sorted([])):\n",
    "    plot_num(grp_var = _col, num_vari = num_vari, hue_var='target',\n",
    "             data = train2,\n",
    "             title_text = str(_iter+1) + '. ' + _col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437a9a97",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## 교호작용항"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c494aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if INTERACTION is True:\n",
    "    train3 = interaction_term(train2,num_vari)\n",
    "    test3  = interaction_term(test2 ,num_vari)\n",
    "\n",
    "    for int_var in train3.columns[[col.find('*')>0 for col in train3.columns]]:\n",
    "        COL_TYPE[int_var] = int\n",
    "else:\n",
    "    train3 = train2.copy()\n",
    "    test3  = test2 .copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14b878b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9033d512",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # (1) onehot encoding\n",
    "# def onehot_encoding(data, col_types):\n",
    "\n",
    "#     raw_data = data.copy()\n",
    "    \n",
    "#     cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "#     for col in cols:\n",
    "#         if col_types[col]==str:\n",
    "\n",
    "#             data = pd.concat([\n",
    "#                 data.drop([col],axis=1).reset_index(drop=True),\n",
    "#                 pd.get_dummies(data[col], prefix = col).reset_index(drop=True).apply(lambda x:x.astype(int))\n",
    "#                 ],\n",
    "#                 axis=1)\n",
    "    \n",
    "#     return(data)\n",
    "\n",
    "# (2) str들 모두 int/category로 바꾸기\n",
    "def str_convert(data, col_types, convert = [int,'category']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            data[col] = data[col].astype(convert)\n",
    "    \n",
    "    return(data)\n",
    "\n",
    "train4 = str_convert(train3, col_types = rmkey(COL_TYPE,'target'), convert = 'category')\n",
    "test4  = str_convert(test3 , col_types = rmkey(COL_TYPE,'target'), convert = 'category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76d0890",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "># **Category Level Check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7925bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하나의 데이터셋에 대해서 level 개수를 search\n",
    "def check_category(data,col_types, ret=['dict','list']):\n",
    "\n",
    "    cols = list(set(data.columns) - set(['target']))\n",
    "\n",
    "    if ret=='dict':\n",
    "        len_cate = {}\n",
    "    elif ret=='list':\n",
    "        len_cate = []\n",
    "    else:\n",
    "        raise('error ret')\n",
    "        \n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            _len = len(data[col].value_counts().index)\n",
    "            \n",
    "            if ret=='dict':\n",
    "                len_cate[col] = _len\n",
    "            elif ret=='list':\n",
    "                len_cate.append(_len)\n",
    "            \n",
    "    return(len_cate)\n",
    "\n",
    "check_category(train4,COL_TYPE,ret='dict')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37621b1b",
   "metadata": {},
   "source": [
    "##### train에서 모두 2개 이상으로, 이상없음\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60882259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개 데이터셋의 동일한 변수에 대해서, level이 같은지 확인\n",
    "def check_category2(data1,data2,col_types):\n",
    "\n",
    "    cols = list(set(data1.columns) - set(['target']))\n",
    "    max_char_len = max([len(x) if col_types[x]==str else 0 for x in col_types.keys()])\n",
    "    \n",
    "    # 없음\n",
    "    for col in cols:\n",
    "        if col_types[col]==str:\n",
    "            data1_cate = data1[col].value_counts().index.values.sort_values()\n",
    "            data2_cate = data2[col].value_counts().index.values.sort_values()\n",
    "            n_blank = (max_char_len-len(col))\n",
    "            if len(data1_cate)==len(data2_cate):\n",
    "                same_index =  data1_cate != data2_cate\n",
    "                print(col, ' '*n_blank, ':', same_index.sum())\n",
    "            else:\n",
    "                print(col, ' '*n_blank, ': differ length')\n",
    "            \n",
    "print(color.BOLD + color.BLUE + '> 다른 카테고리의 개수' + color.END)\n",
    "check_category2(train4,test4,COL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927410c4",
   "metadata": {},
   "source": [
    "##### 카테고리가 match 확인\n",
    "<br></br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04617ab",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc11ead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_var = list(set([col for col in COL_TYPE.keys() if COL_TYPE[col]!=str]) - set(['id']))\n",
    "# len(scale_var),len(np.unique(scale_var)[0])\n",
    "\n",
    "# 모두 0 이상의 값을 가짐\n",
    "# min, max가 test에서 bound를 벗어나는게 있긴함..\n",
    "print(color.BOLD + color.BLUE + '> train' + color.END)\n",
    "for var in scale_var:\n",
    "    print(minmax(train4[var]))\n",
    "\n",
    "print('\\n' + color.BOLD + color.BLUE + '> test ' + color.END)\n",
    "for var in scale_var:\n",
    "    print(minmax(test4 [var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "if SCALE:\n",
    "\n",
    "    # Normalization\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train4[scale_var])\n",
    "    # print(scaler.n_samples_seen_, scaler.data_min_, scaler.data_max_, scaler.feature_range)\n",
    "\n",
    "    train5 = train4.copy()\n",
    "    test5  = test4 .copy()\n",
    "\n",
    "    train5[scale_var] = scaler.transform(train5[scale_var])\n",
    "    test5 [scale_var] = scaler.transform(test5 [scale_var])\n",
    "    \n",
    "    print(color.BOLD + color.BLUE + '> Min Max Scaling Ratio' + color.END)\n",
    "    print(test5[scale_var].apply(lambda x: minmax(x)))\n",
    "    \n",
    "else:\n",
    "    train5 = train4.copy()\n",
    "    test5  = test4. copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca456318",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "<br></br>\n",
    "># **Modelling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8aaa3e",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM setting with Bayesian Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac126d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayesian optimization에 쓰일 hyper parameter들의 boundary\n",
    "bounds_LGB = {\n",
    "    'num_leaves': (100, 800), \n",
    "    'min_data_in_leaf': (0, 150),\n",
    "    'bagging_fraction' : (0.3, 0.9),\n",
    "    'feature_fraction' : (0.3, 0.9),\n",
    "    'min_child_weight': (0.01, 1.),   \n",
    "    'reg_alpha': (0.01, 1.), \n",
    "    'reg_lambda': (0.01, 1),\n",
    "    'max_depth':(6, 23),\n",
    "    'learning_rate': (1e-8, 0.05)\n",
    "}\n",
    "\n",
    "# bayesian optimazation을 통하여 hyper parameter를 선택한\n",
    "# lightgbm modelling\n",
    "def build_lgb(x, y, val_x, val_y,\n",
    "              init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "              ret_param=True, verbose=-1, is_test=False, \n",
    "              SEED=SEED):\n",
    "    \n",
    "    # verbose : 2 항상 출력, verbose = 1 최댓값일 때 출력, verbose = 0 출력 안함\n",
    "    \n",
    "    # (1) 각 hyper parameter들의 lgb model의 f1 score를 return\n",
    "    def LGB_bayesian(\n",
    "        num_leaves, \n",
    "        bagging_fraction,\n",
    "        feature_fraction,\n",
    "        min_child_weight, \n",
    "        min_data_in_leaf,\n",
    "        max_depth,\n",
    "        reg_alpha,\n",
    "        reg_lambda,\n",
    "        learning_rate,\n",
    "         ):\n",
    "        # LightGBM expects next three parameters need to be integer. \n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "\n",
    "        assert type(num_leaves) == int\n",
    "        assert type(min_data_in_leaf) == int\n",
    "        assert type(max_depth) == int\n",
    "\n",
    "        params = {\n",
    "            'num_leaves': num_leaves, \n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'min_child_weight': min_child_weight,\n",
    "            'bagging_fraction' : bagging_fraction,\n",
    "            'feature_fraction' : feature_fraction,\n",
    "            'learning_rate' : learning_rate,\n",
    "            'max_depth': max_depth,\n",
    "            'reg_alpha': reg_alpha,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'objective': 'binary',\n",
    "            'save_binary': True,\n",
    "            'seed': SEED,\n",
    "            'feature_fraction_seed': SEED,\n",
    "            'bagging_seed': SEED,\n",
    "            'drop_seed': SEED,\n",
    "            'data_random_seed': SEED,\n",
    "            'boosting': 'gbdt', \n",
    "            'verbose': -1,\n",
    "            'boost_from_average': True,\n",
    "            'metric':METRIC,\n",
    "            'n_estimators': N_ESTIMATORS, # 1000\n",
    "            'n_jobs': -1,\n",
    "        }    \n",
    "\n",
    "        ## set reg options\n",
    "        model = lgb.LGBMClassifier(**params)\n",
    "        model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=30, verbose=-1)\n",
    "        pred = model.predict(val_x)\n",
    "        score = f1_score(val_y, pred)\n",
    "        return score\n",
    "    \n",
    "    # (2) Get hyper parameter by bayesian optimazation\n",
    "    optimizer = BayesianOptimization(LGB_bayesian, bounds_LGB, random_state=SEED, verbose=-1)\n",
    "    \n",
    "    # initial point, n_iter에 대해서 maximize 하는 bayesian optimazation 실행\n",
    "    optimizer.maximize(init_points=init_points, n_iter=n_iter, \n",
    "                       acq='ei', xi=0.01)\n",
    "    # init_points는 처음 탐색 횟수. \n",
    "    # pbound에서 설정한 구간 내에서 init_points 만큼 입력값을 샘플링하여 계산이 진행\n",
    "    # n_iter은 연산 횟수입니다. 따라서 총 25번을 수행\n",
    "    # xi는 exploration-explotation의 강도를 조절하는 인수로 일반적으로 0.01로 설정하여 exploration을 높여줌\n",
    "    \n",
    "    \n",
    "    # (3) bayesian optimazation를 통해서 얻은 hyper parameter\n",
    "    param_lgb = {\n",
    "        'min_data_in_leaf': int(optimizer.max['params']['min_data_in_leaf']), \n",
    "        'num_leaves': int(optimizer.max['params']['num_leaves']), \n",
    "        'learning_rate': optimizer.max['params']['learning_rate'],\n",
    "        'min_child_weight': optimizer.max['params']['min_child_weight'],\n",
    "        'bagging_fraction': optimizer.max['params']['bagging_fraction'], \n",
    "        'feature_fraction': optimizer.max['params']['feature_fraction'],\n",
    "        'reg_lambda': optimizer.max['params']['reg_lambda'],\n",
    "        'reg_alpha': optimizer.max['params']['reg_alpha'],\n",
    "        'max_depth': int(optimizer.max['params']['max_depth']), \n",
    "        'objective': 'binary',\n",
    "        'save_binary': True,\n",
    "        'seed': SEED,\n",
    "        'feature_fraction_seed': SEED,\n",
    "        'bagging_seed': SEED,\n",
    "        'drop_seed': SEED,\n",
    "        'data_random_seed': SEED,\n",
    "        'boosting': 'gbdt', \n",
    "        'verbose': -1,\n",
    "        'boost_from_average': True,\n",
    "        'metric': METRIC, #'auc',\n",
    "        'n_estimators': N_ESTIMATORS, # 1000\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "\n",
    "    # final parameter\n",
    "    params = param_lgb.copy()\n",
    "    \n",
    "    # final model\n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(x, y, eval_set=(val_x, val_y), early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "              callbacks = [lgb.early_stopping(10, verbose=-1), lgb.log_evaluation(period=-1)])\n",
    "    \n",
    "    if ret_param:\n",
    "        return model, params\n",
    "    else:\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6b3e3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### segment and dataset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a523a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ca==cp > exang > slope > sex\n",
    "# > ca는 2,3건수가 너무적음\n",
    "# > cp는 1,3건수가 너무 적음\n",
    "seg_var = ['is_ca','is_cp','sex','exang','slope2']\n",
    "\n",
    "# 각 세그별 최소 건수\n",
    "[train5[seg_var_x].value_counts().min() for seg_var_x in seg_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233309b6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### Initial Value 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c101cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*50)\n",
    "print('   Initial Values')\n",
    "print('-'*50)\n",
    "max_char_len = max([len(var) for var in ini_var])\n",
    "for var in ini_var:\n",
    "    char_len = ' '*(max_char_len-len(var))\n",
    "    print(f'\\t{var} {char_len} : {eval(var)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffdab6",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "## LGBM fitting for each segment category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2de58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "train_df = train5.copy()\n",
    "test_df  = test5 .copy()\n",
    "\n",
    "for seg_var_x in seg_var:\n",
    "\n",
    "    train_df[seg_var_x+'_pred']   = np.nan\n",
    "    test_df [seg_var_x+'_pred']   = np.nan\n",
    "\n",
    "    train_df[seg_var_x+'_tr_idx'] = np.nan\n",
    "\n",
    "    # segment별로 modelling\n",
    "    for iter in range(0,len(train_df[seg_var_x].value_counts().index)):\n",
    "\n",
    "        # segment setting\n",
    "        seg_var_value = train_df[seg_var_x].value_counts().index[iter]\n",
    "\n",
    "        # data setting\n",
    "        if seg_var_x is not None:\n",
    "            tr_seg_df = train_df[train_df[seg_var_x] == seg_var_value]\n",
    "            te_seg_df = test_df [test_df [seg_var_x] == seg_var_value]\n",
    "\n",
    "            drop_var = ['id','target'] +\\\n",
    "            [seg_var_x           for seg_var_x in seg_var] +\\\n",
    "            [seg_var_x+'_pred'   for seg_var_x in seg_var] +\\\n",
    "            [seg_var_x+'_tr_idx' for seg_var_x in seg_var]\n",
    "            \n",
    "            X_train = tr_seg_df[list(set(tr_seg_df.columns)-set(drop_var))]\n",
    "            X_test  = te_seg_df[list(set(te_seg_df.columns)-set(drop_var))]\n",
    "\n",
    "            y_train = tr_seg_df['target'][tr_seg_df[seg_var_x] == seg_var_value].astype(int).values\n",
    "\n",
    "        else:\n",
    "            tr_seg_df = train_df\n",
    "            te_seg_df = test_df\n",
    "\n",
    "            X_train = tr_seg_df.drop(['id','target'],axis=1)\n",
    "            X_test  = te_seg_df.drop(['id'         ],axis=1)\n",
    "\n",
    "            y_train = tr_seg_df['target'].astype(int).values\n",
    "\n",
    "            \n",
    "        # feature importance\n",
    "        model = lgb.LGBMClassifier(seed = SEED)\n",
    "        model.fit(X_train, y_train, verbose=-1)\n",
    "\n",
    "        feature_imp = pd.DataFrame(zip(X_train.columns,\n",
    "                                       model.feature_importances_.astype(float)), \n",
    "                                   columns=['feature','imp']).sort_values(by='imp')\n",
    "            \n",
    "        reduced_var = list(feature_imp.feature[feature_imp.imp>1])\n",
    "\n",
    "        X_train_new = X_train[reduced_var]\n",
    "        X_test_new  = X_test [reduced_var]\n",
    "        \n",
    "        \n",
    "        # modelling\n",
    "        n_fold = 5\n",
    "        sf = StratifiedKFold(n_fold, shuffle=True, random_state=SEED)\n",
    "\n",
    "        y_tr = []\n",
    "        y_te = []\n",
    "\n",
    "        c = 1\n",
    "        for tr_idx, val_idx in sf.split(X_train_new, y_train):\n",
    "            print(len(tr_idx), len(val_idx))\n",
    "            print('#'*25, f'CV {c}')\n",
    "\n",
    "            model, _ = build_lgb(X_train_new.iloc[tr_idx ], y_train[tr_idx ], \n",
    "                                 X_train_new.iloc[val_idx], y_train[val_idx],\n",
    "                                 init_points=INIT_POINTS, n_iter=N_ITER, cv=N_CV, \n",
    "                                 ret_param=True, is_test=False, \n",
    "                                 SEED=SEED)\n",
    "\n",
    "            y_tr_0 = model.predict(X_train_new)\n",
    "            y_te_0 = model.predict(X_test_new)\n",
    "\n",
    "            y_tr.append(y_tr_0)\n",
    "            y_te.append(y_te_0)\n",
    "\n",
    "            c += 1\n",
    "\n",
    "        # seg별 predict값 넣기\n",
    "        train_df[seg_var_x+'_pred'][train_df[seg_var_x] == seg_var_value] = np.where(np.mean(y_tr, 0)>0.5, 1, 0)\n",
    "        test_df [seg_var_x+'_pred'][test_df [seg_var_x] == seg_var_value] = np.where(np.mean(y_te, 0)>0.5, 1, 0)\n",
    "        \n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b19fc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = (end_time-start_time)/60\n",
    "f'{runtime:.2f} Mins'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f9e2b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### seg별 confusion matrix and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f39ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seg_var_x in seg_var:\n",
    "    \n",
    "    tr_pred = train_df[f'{seg_var_x}_pred']\n",
    "    tr_true = train_df.target.astype(int).values\n",
    "    tr_f1   = f1_score(tr_pred,tr_true)\n",
    "    \n",
    "    print('-'*50)\n",
    "    print(f'{seg_var_x} - f1_score : {tr_f1:.2f}')\n",
    "    print(pd.crosstab(tr_pred,tr_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d1e05c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### train_df 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57b0f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.to_csv(OUT_PATH + 'train_df(2).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957e3479",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ef8f4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 각 seg별 f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score_list = []\n",
    "for seg_var_x in seg_var:\n",
    "    f1 = f1_score(train_df[[seg_var_x + '_pred']].astype(int).values,train_df.target.astype(int).values)\n",
    "    n_blank = ' '*(max_char_len-len(seg_var_x))\n",
    "    \n",
    "    if seg_var_x==seg_var[0]: print(' '*max_char_len, 'f1_score')\n",
    "    print(f'{seg_var_x} {n_blank} {f1: .3f}')\n",
    "    \n",
    "    f1_score_list.append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c446a4",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### 각 segment의 weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0215155",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = [f/sum(f1_score_list) for f in f1_score_list]\n",
    "for seg,w in zip(seg_var,weight):\n",
    "    n_blank = ' '*(max_char_len-len(seg))\n",
    "    if seg==seg_var[0]: print(' '*max_char_len, '  weight')\n",
    "    print(f'{seg} {n_blank} {w*100: .1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9417b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### weighted predicted value : [1,0]의 조합이라서 똑같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4c45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) mixed\n",
    "tr_pred_mix = train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)\n",
    "tr_f1_score_mix = f1_score(np.where(tr_pred_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "# (2) weighted mixed\n",
    "tr_pred_weighted_mix = np.array([train_df[seg+'_pred'].astype(int).values*weight[iter] \n",
    "                                 for iter,seg in enumerate(['is_ca', 'is_cp', 'sex', 'exang', 'slope2'])]).sum(axis=0)\n",
    "tr_f1_score_weighted_mix = f1_score(np.where(tr_pred_weighted_mix>0.5,1,0), train_df.target.astype(int).values)\n",
    "\n",
    "print(f'f1_score of          mixed segment predicted value:{tr_f1_score_mix         : .3f}')\n",
    "print(f'f1_score of weighted mixed segment predicted value:{tr_f1_score_weighted_mix: .3f}')\n",
    "\n",
    "# a=pd.DataFrame({'x' : np.where(tr_pred_mix>0.5,1,0),\n",
    "#                 'y' : np.where(tr_pred_weighted_mix>0.5,1,0)})\n",
    "# pd.crosstab(a.x,a.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8bbad9",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### seg들을 조합해서 pred 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tr_pred_mix'] = np.where(train_df[[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')\n",
    "test_df ['te_pred_mix'] = np.where(test_df [[seg + '_pred' for seg in seg_var]].sum(axis=1)/len(seg_var)>0.5, '1', '0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df359e05",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "##### confusion matrix and f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81dafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(train_df['tr_pred_mix'].astype(int).values,train.target.astype(int).values)\n",
    "print(f'f1_score : {f1:.3f}\\n')\n",
    "print(pd.crosstab(train_df['tr_pred_mix'],train.target))\n",
    "\n",
    "' / '.join([f'{var}:{eval(var)}' for var in ini_var])\n",
    "\n",
    "\n",
    "# f1 : 0.899 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "# f1 : 0.859 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:False')\n",
    "\n",
    "# f1 : 0.897 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:False / INTERACTION:True')\n",
    "# f1 : 0.877 ('SEED:777 / INIT_POINTS:15 / N_ITER:15 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:False / INTERACTION:False')\n",
    "\n",
    "# f1 : 0.920 ('SEED:777 / INIT_POINTS:45 / N_ITER:100 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "# f1 : 0.860 ('SEED:777 / INIT_POINTS:50 / N_ITER:200 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:auc / PLOT:False / SCALE:True / INTERACTION:True')\n",
    "\n",
    "# f1 : 0.907 ('SEED:777 / INIT_POINTS:50 / N_ITER:300 / N_CV:4 / EARLY_STOPPING_ROUNDS:30 / N_ESTIMATORS:2000 / METRIC:binary_logloss / PLOT:False / SCALE:True / INTERACTION:True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba92e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['te_pred_mix'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429901fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub['target'] = ['1' if pred=='1' else '0' for pred in test_df['te_pred_mix']]\n",
    "sub.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수 : 0.92537\n",
    "# sub.to_csv(OUT_PATH + 'sample_18.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.read_csv(OUT_PATH + '★sample_9_mixed.csv')\n",
    "b=sub\n",
    "\n",
    "print(pd.crosstab(a.target,b.target))\n",
    "print(f1_score(a.target.astype(int).values,b.target.astype(int).values))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.797px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
